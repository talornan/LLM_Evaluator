# LLM Evaluator: Unleash the Potential of Large Language Models ü™Ñ

## üìñ Project Description
**Stop wasting time on manual LLM evaluation! ‚è∞**  LLM Evaluator automates everything - feed your prompts, choose models, and instantly get deep insights & comparisons across models and tasks.

### **Here's what LLM Evaluator can do for you:**  ‚ö°

**Effortless Evaluation**: Feed your prompts, choose models, and instantly get deep insights and comparisons with key metrics like Fluency, Coherence, and Toxicity. ‚ú®

**Model Matchmaker**: Find the perfect LLM for your task by comparing responses across different models.

**Continuous Improvement**: Track progress over time, identify areas for improvement, and stay ahead of the curve.

**Tailored for Experts**: Whether you're a Prompt Engineer crafting winning prompts or a Model Developer optimizing performance, LLM Evaluator has tools for you.

**In addition to the above, here are some details about the evaluation metrics:**
1. Roughness:ü™® Measures the consistency and correctness of the response.
2. Coherence: Evaluate the logical flow and cohesiveness of the response.
3. Fluency:ü™∂ Assesses the naturalness and smoothness of the response.
4. Toxicity: ‚Äç‚ôÄÔ∏è Detects the presence of harmful or offensive language in the response.

## LLM Evaluator is perfect for:

1. Prompt Engineers:‚ú® Create Prompts, analyze performance, receive recommendations for improving Prompts.‚ú®
2. Model Developers: Analyze model performance, receive recommendations on which model to use for a specific Prompt.

## Technologies: ‚úçÔ∏è 

LLM Evaluator leverages a robust tech stack for seamless evaluation and insightful comparisons:

**Frontend:** HTML, CSS (Built with Streamlit for rapid development)
**Backend:** Python (Fast API for a performant and flexible REST API)
**Machine Learning:** Hugging Face Transformer library for efficient access and interaction with various LLMs
**Database:** MySQL for storing prompts, responses, and evaluation results

**API Functionality:** ‚ú®
**Submit Prompts:** Send your prompts and specify desired LLM models through intuitive API calls.
**Evaluation Results:** Retrieve comprehensive reports including key metrics like Fluency, Coherence, Roughness, and Toxicity (leveraging Hugging Face evaluation functions).
**Model Comparison:** Compare performance across different LLMs for informed decision-making.


**Ready to unleash the full potential of your LLMs?**
**Start your free trial today! ‚ú®** 

